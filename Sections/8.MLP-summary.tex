\hspace{0.5cm} MultiLayer Perceptrons(MLP) are a type of supervised learning \cite{michie1994machine}. It is a type of neural network, which is based on the way the human brain functions. A MLP consists of an input layer, hidden layers and an output layer. To have a better understanding of how this works, let's first look at a (single layer) perceptron\cite{rosenblatt1958perceptron}. A perceptron \ref{fig:perceptron} is a system of nodes with input nodes, called the input layer, and an output node called the output layer. It functions very much like neurons in the brain. Inputs from a vector $x$ are fed to the input layer with weights $w$. The dot product of $x$ and $w$, $\sum$ ,  is fed to an activation function. The output again resembles that of a neuron, in the sense that it "fires" if the activation function passes the threshold value. However, perceptrons cannot solve all problems. To be specific, non separable problems cause problems. MLP can handle problems that are more complex. \\
\begin{figure}[H]
    \includegraphics[width=80mm]{./img/perceptron.png}
    \caption{Rosenblatt's Perceptron \cite{wikiPerceptronPNG}}
    \label{fig:perceptron}
\end{figure}

A MLP works in a similar way, but just repeated many times. The input layer consists of perceptrons. Each perceptron passes its output to the hidden layer. In the hidden layer, a number of perceptrons take these outputs as input and pass their output to the output layer of the MLP. This concept of only passing outputs forward in the network is called "feed forward". Often only one hidden layer is used because any MLP that uses linear activation functions can be reduced to this form. However, non-linear activation functions are also possible. Increasing the number of hidden layers or nodes in layers increases sensitivity but also increases risk of overfitting \cite{Murtagh1991183}.

Learning in the human brain is done by strengthening the connection between neurons. This is mimicked by adjusting the weights in the perceptrons. Something with a desired result will have its weight increased so certain inputs contribute more to the activation function than others.
(more details about learning and MLP in general will come with the first draft).