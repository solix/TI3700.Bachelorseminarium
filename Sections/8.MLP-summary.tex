MultiLayer Perceptrons(MLP) are a type of supervised learning \cite{michie1994machine}. It is a type of neural network, invented in 1969 \cite{minsky1969perceptions}, which at the time  was inspired by the way the human brain functions. A MLP consists of an input layer, hidden layers and an output layer. To have a better understanding of how this works, let's first look at a (single layer) perceptron\cite{rosenblatt1958perceptron}. 

A perceptron \ref{fig:perceptron} is a system of nodes with input nodes, called the input layer, and an output node called the output layer. It functions very much like neurons in the brain. Inputs from a vector $x$ are fed to the input layer with weights $w$. The dot product of $x$ and $w$, $\sum$ ,  is fed to an activation function. The output again resembles that of a neuron, in the sense that it "fires" if the activation function passes the threshold value. This threshold value corresponds to, what in classification, is known as a decision boundary. It represents the difference between classes. However, perceptrons cannot solve all problems. To be specific, non separable problems cause problems. MLP can handle problems that are more complex. \\
\begin{figure}[H]
    \includegraphics[width=80mm]{./img/perceptron.png}
    \caption{Rosenblatt's Perceptron \cite{wikiPerceptronPNG}}
    \label{fig:perceptron}
\end{figure}

A MLP works in a similar way, but just repeated many times. The input layer consists of perceptrons. Each perceptron passes its output to the hidden layer. In the hidden layer, a number of perceptrons take these outputs as input and pass their output to the output layer of the MLP. This concept of only passing outputs forward in the network is called "feed forward". Often only one hidden layer is used since this can already approximate most continues functions \cite{cybenko1989approximation}. However, non-linear activation functions are also possible. Increasing the number of hidden layers or nodes in layers increases sensitivity but also increases risk of overfitting \cite{Murtagh1991183}.

Learning in the human brain is done by strengthening the connection between neurons. This is mimicked by adjusting the weights in the perceptrons. Something with a desired result will have its weight increased so certain inputs contribute more to the activation function than others. 

The MLP has a couple of strong advantages. Firstly, MLP does not make assumptions on the distribution of a dataset. This makes for great generalization and allows MLP to handle new datasets extremely well. As mentioned before, it can also deal with non-linear functions \cite{gardner1998artificial}. This leads to some very clear advantages over other classifiers.