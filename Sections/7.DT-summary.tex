A decision tree is a classifier that makes several sequential decisions. The outcome of that sequence determines whether a data point belongs to one class or another. The structure of a decision tree \ref{fig:DT} is defined as a set of nodes ${x_1, ... , x_n}$ with edges ${e_1, .., e_{m}}$ between them. The edges are directed and there are no cycles in the network. The tree has one root $x_1$. With each data point we start at the root. The root node is fed information about a certain feature. The root node then evaluates a rule or function that allows the node to make a decision about what class a data point probably belongs to. Each decision leads to a different node, where another decision is made, increasing the likelihood that the data point belongs to a certain class. This process repeats itself until one of the leaf nodes is reached \cite{safavian1991survey}. From there we can determine what class our initial data point belonged to (either class $A$, $B$ or $C$ in the figure below. 

\begin{figure}[H]
    \includegraphics[width=80mm]{./img/decisiontree.png}
    \caption{\footnotesize{Decision Tree example. $x_i$ are nodes where decisions are made. $e_j$ are edges leading down the tree to other nodes. $A,B,C$ are classes that instances are to be classified into.}}
    \label{fig:DT}
\end{figure}

Decision trees are generally built or `grown' with a simple algorithm. Of course random selection of features is an option, but there are many other approaches, like the famous ID3 \cite{quinlan1986induction} and C4.5 algorithm \cite{quinlan1993c4}. ID3 roughly works as follows: We create a node that splits the tree into at least 2 branches. To create a node, we compare every feature and split the one with the highest information gain. In other words, the division will give us a lot of information about the data points and their classes. For example: in classifying trees and plants, height might give more information about which class an instance belongs to than the colour of its leaves would. For each of the subsets that comes forth out of that division, the process is repeated until there is enough certainty that the remaining subset represents the class. This does not take into account that a combination of features might make for an even better split, but it is a relatively efficient algorithm. %Another example of a tree building algorithm is called Fisher's Linear Discriminant Tree \cite{LópezChau20136283}. Fisher's tree is based on maximizing the distance between the means of classes. 

A tree stops to grow when the impurity level is low enough. The impurity level is a metric to indicate what the chances are of misclassification. A simple example for any $node_j$ would be $impurity(node_j) = 1 - max(P(z_i = c_a) $. Here $z_i$ is the $i^{th}$ instance of the subset at our node to be classified into class $c_a$. 

The opposite of growing a tree is called `pruning'. This is done to decrease the complexity of the tree and reduce chance of over fitting. The experts may decide to prune a tree as well if, in hindsight, the training data was faulty in some way. Pruning is done by removing the least significant node(s). 

It is also possible to combine multiple decision trees into a Random Forest \cite{breiman2001random}. This allows for higher accuracy as well as handling higher complexity. Features to split the trees are chosen randomly for each tree. To have enough samples to train the forest, bagging can be performed. Bagging takes random samples from a dataset, with replacement. With this, you have more smaller datasets (with duplicates) that you can train on. With enough trees, the variance of the accuracy decreases. Within forests, every instance to be classified is classified by every tree. The class of an instance is determined by majority vote over all the trees. 

In general, the more nodes a tree has, the more accurate it can be trained. The downside is that the time to classify and train will increase the larger the tree becomes, as well as an increased risk in over fitting. The design of the tree is crucial, as each node splits up the dataset in a certain way. Dividing a datasets in the wrong order can make for significantly slower and less accurate trees. The advantage of the decision tree is it's computational efficiency \cite{safavian1991survey}, its intuitive structure and resistance to noise \cite{LópezChau20136283}.