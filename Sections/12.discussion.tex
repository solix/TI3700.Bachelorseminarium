There are some key issues for effectively classifying dark spots, several factors should be taken into account\cite{Kubat:1998:MLD:288808.288812}. The available data containing oil spills is scarce compared to lookalikes. This leads to a very imbalanced dataset. As a consequence, a classifier sensitive to this problem can not reach it's maximum accuracy\cite{Japkowicz20026}. SVM seem the least affected by this imbalance. Validity of the data selection, there is no guarantee that the data used for training the classifier are representatives of future samples, espescially with such a scarce dataset as in oil spill detection. The feature selection procedure should be done with care as it can influence prediction accuracy. Out of the 25 common features, most researchers arbitraly select a subset of features and compare multiple subsets using cross-validation, a technique for assessing how well a model generalizes to an independent dataset. Finally, a classifier should be chosen with all the issues mention in mind including the interpretablity and training times if it should be used in a reactive decision support system. In the table below, all previous mentioned studies are shown including their results and characteristics. This should allow a better discussion on which classifier is well suited for oil spill detection.

\begin{table*}[t]

\advance\leftskip-3cm
\setstretch{1.5}
\tabcolsep=0.19cm
\footnotesize

\adjustbox{max width=0.97\pdfpagewidth, left}{

\begin{tabular}{*{6}{c}}
  Study & data type & Preprocessing & \# Features & formations & Results \\
    \hline
  
    Hydro-acoustics & Sonar & Echoview & 15 &  - & SVM accuracy: $89.5$\%, DT accuracy: $86.8$\% \\

    Land coverage1986 & LandSat SAR & "Using data miner" & 11 &  - & SVM max accuracy: $90.53$\%, DT accuracy: $93.48$\% \\

    Land coverage2001 & LandSat SAR & "Using data miner" & 10 &  - & SVM max accuracy: $93.67$\%, DT accuracy: $94.07$\% \\ 
    
    Oil spills\cite{Topouzelis200762} & ERS-2 SAR, 24 high-res images 8-bit & transformation, Filtering, data normalization & 10 & 90 lookalikes and 69 oil spills & MLP(10:51:2) accuracy: $86.67$\% lookalike acc. $91.18$\% oil spills acc.\\
    
    Oil spills\cite{Delfrate200038} & ERS SAR, 600 low-res images & Resampling,Radiometric range correction, georeference & 11 & 68 lookalikes and 71 oil spills & MLP(11-8-4-1) accuracy: $90$\% lookalike acc. $82$\% oil spills acc. [leave-one-out approach]\\
    
    Oil spills\cite{Topouzelis200930} &  ERS-2 SAR, 24 high-res images & - & 10 & 90 lookalikes and 69 oil spills & MLP(10-51-2) accuracy: $84.4$\% lookalike acc. $85.3$\% oil spills acc.\\
    
    Oil spills\cite{Topouzelis200924} &  ERS SAR, 12 high-res images & 8-bit transformation, filtering & - & - & MLP(4-2-1) accuracy: $96.46$\% overall acc. \\
 
    Oil spills\cite{Delfrate2004} &  ERS SAR, 70 images & - & 12 & 78 lookalikes and 111 oil spills & MLP(12-8-8-1) 0.227 root mean square error(rmse)\\
    
    Oil spills\cite{Xu201414} &  RADARSAT-1, 93 images & log-transformation, standardization & 15 & 94 lookalikes and 98 oil spills & MLP $75.93$\% overall acc. SVM $79.63$\% overall acc. DT(Bundling) 90.74 overall acc.\\
    
    Oil spills\cite{Mera201472} &  Envisat, 47 images & - & 9 & 155 lookalikes and 80 oil spills & MLP(9:11:2) $96.3$\% lookalike acc. $92.9$\% oil spill acc. \\
    & & & & & DT $92.6$\% lookalike acc. $92.9$\% oil spill acc. \\
    
    Oil spills\cite{Delfrate1996} &  ERS-1 SAR, 59 low-res images & - & 9 & 2471 lookalikes and 42 oil spills & DT $96$\% lookalike acc. $86$\% oil spill acc\\
    
    Oil spills\cite{Topouzelis201268} &  ERS-2 SAR, 24 high-res images & - & 9 & 90 lookalikes and 69 oil spills & DT forest $84.4$\%\\ 
    
    Oil spills\cite{brekke2008classifiers} & ENVISAT, 103 images & masking & - & 12244 lookalikes and 41 oil spills & SVM(C-SVC) $77.4$\% lookalike acc. $82.9$\% oil spill acc.\\

    ECG arrhythmias\cite{Moavenian20103088} & MIT-BIH arrhythmia database & - & 10 & - & accuracy SVM 99\% , MLP 98.22\% \\

    Remote Sensing\cite{Zanaty2012177} & Satimage & feature extraction & 26& - & accuracy for SVM 93.16\% ,and for MLP 96.98\%\\
    
    signature recognition\cite{FriasMartinez2006693} & user signature data &feature extraction& 2 & - & Recognition rate SVM 66.5  , MLP 71.2\\
    wind speed prediction\cite{Mohandes2004939}& daily wind speed data & - &high dimensional feature & - & MSE on testing data for the MLP is 0.0090 while it is 0.0078 for the SVM\\
    
    
\end{tabular}
}
\end{table*}

These studies reported varying results in accuracy. Studies not related to oil spills have similar performance. When looking at the table, it is hard and inappropriate to directly compare classifiers in terms of accuracy since they all use a different dataset. Accuracies up to 96\% are claimed\cite{Topouzelis200924}, but without even specifying the data used for training and testing, these results are highly questionable. A direct comparison using the same dataset has al so been done\cite{Mera201472}\cite{Xu201414}. Even if classifiers are trained with the same data, the validity of data issue persists. Since real world samples are scarcly available, different results may be found when using the same methodology on a different dataset. All researchers would benefit from a common database for better accuracy estimation of a classifier\cite{Topouzelis200810}.

In section 4, an attempt to compare the three classifiers has been made including similar experiments that are comparable to oil spill detection. These papers used a similar amount of features that where extracted in scarcely available images. \\ Not all papers agreed on a particular classifier having the highest accuracy, but this could have been expected since the results are under discussion and can't really be compared. Each experiment uses a different data set, some even included unverified samples. The chosen classifier largely depends on the feature dimension, type of features and the data set. Even for seemingly similar problems this can lead to the usage of a different classifier. In the case of oil spill detection, much improvement in the research about different classifiers can be done by establishing a common database with labelled dark spots that are verified. Preferably, dark spots in all shapes, sizes and on different geographical points. This will allow researches to train a classifier with the same data set.